{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "091ca50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections\n",
    "from IPython.display import display, Markdown, Latex\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "from gensim.models import FastText\n",
    "\n",
    "with open('Emotional_BQE_Dict.pickle', 'rb') as fr:\n",
    "    bpe_codes = pickle.load(fr)\n",
    "    \n",
    "with open('Emotional_WordToIndex.pickle', 'rb') as fr:\n",
    "    Emo_WordToIndex = pickle.load(fr)\n",
    "    \n",
    "with open('NER_WordToIndex.pickle', 'rb') as fr:\n",
    "    NER_WordToIndex = pickle.load(fr)\n",
    "    \n",
    "with open('Index_To_NER.pickle', 'rb') as fr:\n",
    "    index_to_NER= pickle.load(fr)\n",
    "\n",
    "Emo_model = load_model('Emo_class_model.h5')\n",
    "Emo_FT = FastText.load('Emo_FT.model')\n",
    "NER_model = load_model('NER_class_model.h5')\n",
    "NER_FT = FastText.load('NER_FT.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dd9062a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word.\n",
    "    Word is represented as a tuple of symbols (symbols being variable-length strings).\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def BPE_encode(orig):\n",
    "    \"\"\"Encode word based on list of BPE merge operations, which are applied consecutively\"\"\"\n",
    "\n",
    "    word = tuple(orig) + ('</w>',)\n",
    "    # display(Markdown(\"__word split into characters:__ <tt>{}</tt>\".format(word)))\n",
    "\n",
    "    pairs = get_pairs(word)    \n",
    "\n",
    "    if not pairs:\n",
    "        return orig\n",
    "\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        # display(Markdown(\"__Iteration {}:__\".format(iteration)))\n",
    "\n",
    "        # print(\"bigrams in the word: {}\".format(pairs))\n",
    "        bigram = min(pairs, key = lambda pair: bpe_codes.get(pair, float('inf')))\n",
    "        # print(\"candidate for merging: {}\".format(bigram))\n",
    "        if bigram not in bpe_codes:\n",
    "            # display(Markdown(\"__Candidate not in BPE merges, algorithm stops.__\"))\n",
    "            break\n",
    "        first, second = bigram\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            try:\n",
    "                j = word.index(first, i)\n",
    "                new_word.extend(word[i:j])\n",
    "                i = j\n",
    "            except:\n",
    "                new_word.extend(word[i:])\n",
    "                break\n",
    "\n",
    "            if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                new_word.append(first+second)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        new_word = tuple(new_word)\n",
    "        word = new_word\n",
    "        # print(\"word after merging: {}\".format(word))\n",
    "        if len(word) == 1:\n",
    "            break\n",
    "        else:\n",
    "            pairs = get_pairs(word)\n",
    "\n",
    "    # 특별 토큰인 </w>는 출력하지 않는다.\n",
    "    if word[-1] == '</w>':\n",
    "        word = word[:-1]\n",
    "    elif word[-1].endswith('</w>'):\n",
    "        word = word[:-1] + (word[-1].replace('</w>',''),)\n",
    "\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aee1a8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "from konlpy.tag import Kkma\n",
    "from eunjeon import Mecab\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "mecab = Mecab()\n",
    "\n",
    "def tokenizing(inputText):\n",
    "    tokenized_sentence = []\n",
    "    \n",
    "    tagged_Text = mecab.pos(inputText)\n",
    "    \n",
    "    for tag, pos in tagged_Text:\n",
    "        tokenized_sentence.append(tag)\n",
    "        \n",
    "    return tokenized_sentence\n",
    "    \n",
    "def Int_encode(inputList, FT_model, vocab):\n",
    "    templist=[]\n",
    "    \n",
    "    for word in inputList:\n",
    "        if word not in vocab:\n",
    "            similars = FT_model.wv.most_similar(word)\n",
    "            tag, value = similars[0]\n",
    "            if value < 0.8:  # 유사성이 심하게 낮을 때 BPE로 subword로 분리해버린 다음에 인코딩\n",
    "                words = BPE_encode(word)\n",
    "                for word in words:\n",
    "                    if word in vocab:\n",
    "                        templist.append(vocab[word])\n",
    "                        \n",
    "            else:\n",
    "                templist.append(vocab[tag])\n",
    "        \n",
    "        else:        \n",
    "            templist.append(vocab[word])\n",
    "            \n",
    "    return templist\n",
    "\n",
    "def PreProcessing(InputText, FT_model, vocab):\n",
    "    tokenized_Text = tokenizing(InputText)\n",
    "    encoded_Text = Int_encode(tokenized_Text, FT_model, vocab)\n",
    "    \n",
    "    return encoded_Text\n",
    "\n",
    "def Predict(Processed_Text, model):\n",
    "    Input = np.array([Processed_Text])\n",
    "    result = model.predict(Input)\n",
    "    \n",
    "    return list(result[0])\n",
    "    \n",
    "def EmotionClassification(InputText, model, FT_model, vocab):\n",
    "    processed = PreProcessing(InputText, FT_model, vocab)\n",
    "    predicted = Predict(processed, model)\n",
    "    Label = \"\"\n",
    "    \n",
    "    if predicted[0] > predicted[1]:\n",
    "        Label = \"부정\"\n",
    "    else:\n",
    "        Label = \"긍정\"\n",
    "            \n",
    "    return Label\n",
    "\n",
    "def NERClassification(InputText, model, FT_model, vocab):\n",
    "    tokenized = tokenizing(InputText)\n",
    "    encoded = temp_int_encode(tokenized, FT_model, vocab)\n",
    "    predicted = Predict(encoded, model)\n",
    "    y_predicted = np.argmax(predicted, axis=-1) # 확률 벡터를 정수 인코딩으로 변경함\n",
    "    \n",
    "    Ner_list = []\n",
    "    \n",
    "    for i in range(len(y_predicted)):\n",
    "        if y_predicted[i] != 1:\n",
    "            Ner_tuple = (tokenized[i], index_to_NER[y_predicted[i]])\n",
    "            Ner_list.append(Ner_tuple)\n",
    "    \n",
    "    return Ner_list\n",
    "    \n",
    "def temp_int_encode(inputList, FT_model, vocab):\n",
    "    templist=[]\n",
    "    \n",
    "    for word in inputList:\n",
    "        if word not in vocab:\n",
    "            similars = FT_model.wv.most_similar(word)\n",
    "            tag, value = similars[0]\n",
    "            \n",
    "            templist.append(vocab[tag])\n",
    "            \n",
    "        else:        \n",
    "            templist.append(vocab[word])\n",
    "            \n",
    "    return templist\n",
    "\n",
    "def CreateDataStruct(name, InputText, Emo_model, Emo_FT, NER_model, NER_FT, Emo_vocab, NER_vocab):\n",
    "    Emo_result = EmotionClassification(InputText, Emo_model, Emo_FT, Emo_vocab)\n",
    "    NER_result = NERClassification(InputText, NER_model, NER_FT, NER_vocab)\n",
    "    \n",
    "    print(\"\\n내담자 : {0}\\n입력문장 : {1}\\n[\\n  감정정보 : {2}\\n  개체명 인식 : {3}\\n]\"\\\n",
    "          .format(name, InputText, Emo_result, NER_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6c9e26ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "내담자 : 김수진\n",
      "입력문장 : 지난 해에 범죄로 확정된 건은 15건이고 미해결된게 1700여건이면 문제 심각한거같은데\n",
      "[\n",
      "  감정정보 : 부정\n",
      "  개체명 인식 : [('지난', 'DAT_B'), ('해', 'DAT_I'), ('15', 'NUM_B'), ('1700', 'NUM_B')]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "testText = \"지난 해에 범죄로 확정된 건은 15건이고 미해결된게 1700여건이면 문제 심각한거같은데\"\n",
    "counselee = \"김수진\"\n",
    "# print(EmotionClassification(testText, Emo_model, Emo_FT, Emo_WordToIndex))\n",
    "# print(NERClassification(testText, NER_model, NER_FT, NER_WordToIndex))\n",
    "\n",
    "CreateDataStruct(counselee, testText, Emo_model, Emo_FT, NER_model, NER_FT, Emo_WordToIndex, NER_WordToIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732a345c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e588e83a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
